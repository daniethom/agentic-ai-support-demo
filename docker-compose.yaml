version: "3.9"

services:
  llm:
    image: ollama/ollama:latest
    container_name: ollama-llm
    ports:
      - "11434:11434"  # Ollama default port
    volumes:
      - ollama:/root/.ollama
    tty: true
    restart: unless-stopped

  weaviate:
    image: semitechnologies/weaviate:latest
    container_name: weaviate
    ports:
      - "8081:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      DEFAULT_VECTORIZER_MODULE: "none"
      ENABLE_MODULES: "none"
    volumes:
      - weaviate_data:/var/lib/weaviate
    restart: unless-stopped

  api-services:
    build: ./api
    container_name: support-api
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
      - "8003:8003"
    volumes:
      - ./api:/app
    depends_on:
      - weaviate
    restart: unless-stopped

  app:
    build: .
    container_name: ai-support-demo
    ports:
      - "8501:8501"  # Streamlit
    environment:
      - LITELLM_CONFIG_PATH=/app/litellm.config.json
    volumes:
      - .:/app
    depends_on:
      - llm
      - weaviate
    restart: unless-stopped

volumes:
  ollama:
  weaviate_data:
