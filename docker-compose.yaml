version: "3.9"

services:
  ollama-llm:
    image: ollama/ollama
    container_name: ollama-llm
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped
    command: ["ollama", "serve"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5

  weaviate:
    image: semitechnologies/weaviate:latest
    container_name: weaviate
    ports:
      - "8081:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      DEFAULT_VECTORIZER_MODULE: "none"
      ENABLE_MODULES: "none"
    volumes:
      - weaviate_data:/var/lib/weaviate
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/meta"]
      interval: 30s
      timeout: 10s
      retries: 5

  api-services:
    build: ./api
    container_name: support-api
    ports:
      - "8000:8000"
    depends_on:
      - weaviate
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  app:
    build: .
    container_name: ai-support-demo
    ports:
      - "8501:8501"
    environment:
      - LITELLM_CONFIG_PATH=/app/litellm.config.json
    volumes:
      - .:/app
    depends_on:
      - ollama-llm
      - weaviate
      - api-services
    restart: unless-stopped

volumes:
  ollama_models:
  weaviate_data: